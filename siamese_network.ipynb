{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation: \n",
    "unpack train and dev set and set expoleded version in apposite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data from original datasets provided in the github repo for the task\n",
    "df_arguments = pd.read_csv('./KPA_2021_shared_task/kpm_data/arguments_train.csv')[['arg_id', 'argument']]\n",
    "df_key_points = pd.read_csv('./KPA_2021_shared_task/kpm_data/key_points_train.csv')[['key_point_id', 'key_point']]\n",
    "df_labels = pd.read_csv('./KPA_2021_shared_task/kpm_data/labels_train.csv')\n",
    "\n",
    "# arrays for data\n",
    "labels = [None] * len(df_labels)  # labels\n",
    "args = [None] * len(df_labels)  # arguments\n",
    "kps = [None] * len(df_labels)  # key points\n",
    "\n",
    "# fill the arrays. \n",
    "for i in range(len(df_labels)):\n",
    "    label = df_labels.iloc[i]\n",
    "\n",
    "    args[i] = df_arguments[df_arguments['arg_id'] == label['arg_id']]['argument'].array[0]\n",
    "    kps[i] = df_key_points[df_key_points['key_point_id'] == label['key_point_id']]['key_point'].array[0]\n",
    "\n",
    "# in this setting we want minimise the cosine similarity of encodings of related sentences,\n",
    "# the desired otput will be values in the domain of the cosine-similarity function:\n",
    "# cos. sim. = 1  if two vectors (encodings) are different,\n",
    "#           = -1 if two vectors are similar\n",
    "labels = list(map(lambda x: 0 if x == 0 else 1, df_labels['label']))\n",
    "\n",
    "# now that we recontructed the full, preprocessed, training dataset, we save it in apposite files\n",
    "args = pd.DataFrame(args)\n",
    "args.to_csv('./args_train.csv')\n",
    "\n",
    "kps = pd.DataFrame(kps)\n",
    "kps.to_csv('./kps_train.csv')\n",
    "\n",
    "labels = pd.DataFrame(labels)\n",
    "labels.to_csv('./labels_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# read the data from original datasets provided in the github repo for the task\n",
    "df_arguments = pd.read_csv('./KPA_2021_shared_task/kpm_data/arguments_dev.csv')[['arg_id', 'argument']]\n",
    "df_key_points = pd.read_csv('./KPA_2021_shared_task/kpm_data/key_points_dev.csv')[['key_point_id', 'key_point']]\n",
    "df_labels = pd.read_csv('./KPA_2021_shared_task/kpm_data/labels_dev.csv')\n",
    "\n",
    "# arrays for data\n",
    "labels = [None] * len(df_labels)  # labels\n",
    "args = [None] * len(df_labels)  # arguments\n",
    "kps = [None] * len(df_labels)  # key points\n",
    "\n",
    "# fill the arrays. \n",
    "for i in range(len(df_labels)):\n",
    "    label = df_labels.iloc[i]\n",
    "\n",
    "    args[i] = df_arguments[df_arguments['arg_id'] == label['arg_id']]['argument'].array[0]\n",
    "    kps[i] = df_key_points[df_key_points['key_point_id'] == label['key_point_id']]['key_point'].array[0]\n",
    "\n",
    "# in this setting we want minimise the cosine similarity of encodings of related sentences,\n",
    "# the desired otput will be values in the domain of the cosine-similarity function:\n",
    "# cos. sim. = 1  if two vectors (encodings) are different,\n",
    "#           = -1 if two vectors are similar\n",
    "labels = list(map(lambda x: 0 if x == 0 else 1, df_labels['label']))\n",
    "# now that we recontructed the full, preprocessed, training dataset, we save it in apposite files\n",
    "\n",
    "\n",
    "args = pd.DataFrame(args)\n",
    "args.to_csv('./args_dev.csv')\n",
    "\n",
    "kps = pd.DataFrame(kps)\n",
    "kps.to_csv('./kps_dev.csv')\n",
    "\n",
    "labels = pd.DataFrame(labels)\n",
    "labels.to_csv('./labels_dev.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here\n",
    "by setting gpus memory growth and re-reading previously prepared datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "###### SET GPU: WHICH ONE TO USE AND MEMORY GROWTH ######\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    print(\"hello!\")\n",
    "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-read the training dataset from the relative files \n",
    "import pandas as pd \n",
    "kps_train = pd.read_csv('./kps_train.csv').to_numpy()[:,1]\n",
    "args_train = pd.read_csv('./args_train.csv').to_numpy()[:,1]\n",
    "labels_train = pd.read_csv('./labels_train.csv').to_numpy()[:,1]\n",
    "\n",
    "kps_dev = pd.read_csv('./kps_dev.csv').to_numpy()[:,1]\n",
    "args_dev = pd.read_csv('./args_dev.csv').to_numpy()[:,1]\n",
    "labels_dev = pd.read_csv('./labels_dev.csv').to_numpy()[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per calcolare max len\n",
    "# max(list(map(lambda x:len(x), args))) , max(list(map(lambda x:len(x), kps)))\n",
    "MAX_LEN = 250\n",
    "tokenized_args_train = tokenizer(args_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_train = tokenizer(kps_train.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "\n",
    "tokenized_args_dev = tokenizer(args_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')\n",
    "tokenized_kps_dev = tokenizer(kps_dev.tolist(), max_length=MAX_LEN, return_tensors='tf', padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "(20635, 250) (20635, 250) (20635, 250) (20635, 250)\n",
      "validation/dev data\n",
      "(3458, 250) (3458, 250) (3458, 250) (3458, 250)\n"
     ]
    }
   ],
   "source": [
    "## TRAINING \n",
    "toks1_input_train = tokenized_args_train.input_ids\n",
    "atts1_input_train = tokenized_args_train.attention_mask\n",
    "\n",
    "toks2_input_train = tokenized_kps_train.input_ids\n",
    "atts2_input_train = tokenized_kps_train.attention_mask\n",
    "\n",
    "print('training data')\n",
    "print(toks1_input_train.shape,atts1_input_train.shape,toks2_input_train.shape,atts2_input_train.shape)\n",
    "\n",
    "\n",
    "## VALIDATION/DEV\n",
    "toks1_input_dev = tokenized_args_dev.input_ids\n",
    "atts1_input_dev = tokenized_args_dev.attention_mask\n",
    "\n",
    "toks2_input_dev = tokenized_kps_dev.input_ids\n",
    "atts2_input_dev = tokenized_kps_dev.attention_mask\n",
    "\n",
    "print('validation/dev data')\n",
    "print(toks1_input_dev.shape,atts1_input_dev.shape,toks2_input_dev.shape,atts2_input_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ` people reach their limit when it comes to their quality of life and should be able to end th\n",
      "[CLS] assisted suicide gives dignity to the person that wants to commit it [SEP] [PAD] [PAD] [PAD] [\n",
      "0\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] assisted suicide allows people to solicit someone to die to their own benefit [SEP] [PAD] [PAD\n",
      "0\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] assisted suicide is akin to killing someone [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [P\n",
      "0\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] assisted suicide should not be allowed because many times people can still get better [SEP] [P\n",
      "1\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] assisted suicide violates the doctor's role [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [P\n",
      "0\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] people do not have the right to self - harm / harm others [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "0\n",
      "[CLS] a cure or treatment may be discovered shortly after having ended someone's life unnecessarily.\n",
      "[CLS] people who are ill do not have clear judgement and are in need of help [SEP] [PAD] [PAD] [PAD]\n",
      "0\n",
      "[CLS] a patient should be able to decide when they have had enough \" care \". [SEP] [PAD] [PAD] [PAD]\n",
      "[CLS] assisted suicide gives dignity to the person that wants to commit it [SEP] [PAD] [PAD] [PAD] [\n",
      "0\n",
      "[CLS] a patient should be able to decide when they have had enough \" care \". [SEP] [PAD] [PAD] [PAD]\n",
      "[CLS] assisted suicide reduces suffering [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "0\n",
      "[CLS] a patient should be able to decide when they have had enough \" care \". [SEP] [PAD] [PAD] [PAD]\n",
      "[CLS] people should have the freedom to choose to end their life [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print( tokenizer.decode(toks1_input_train[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_train[i])[0:100] )\n",
    "  print(labels_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a real education is about giving students the tools to learn, think, and express themselves ; \n",
      "[CLS] school uniform is harming the student's self expression [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [P\n",
      "1\n",
      "[CLS] a real education is about giving students the tools to learn, think, and express themselves ; \n",
      "[CLS] school uniforms are expensive [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD\n",
      "0\n",
      "[CLS] a real education is about giving students the tools to learn, think, and express themselves ; \n",
      "[CLS] school uniforms are often uncomfortable / sexist [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PA\n",
      "0\n",
      "[CLS] children express themselves through the clothes they wear and should be able to do this at sch\n",
      "[CLS] school uniform harms learning / creativity [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PA\n",
      "0\n",
      "[CLS] children express themselves through the clothes they wear and should be able to do this at sch\n",
      "[CLS] school uniform is harming the student's self expression [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [P\n",
      "1\n",
      "[CLS] children express themselves through the clothes they wear and should be able to do this at sch\n",
      "[CLS] school uniforms are expensive [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD\n",
      "0\n",
      "[CLS] children express themselves through the clothes they wear and should be able to do this at sch\n",
      "[CLS] school uniforms are often uncomfortable / sexist [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PA\n",
      "0\n",
      "[CLS] children express themselves through the clothes they wear and should be able to do this at sch\n",
      "[CLS] school uniforms increase conformity or harm individuality [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "0\n",
      "[CLS] children should be able to dress as they wish, within reason, at school rather than being rest\n",
      "[CLS] school uniform harms learning / creativity [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PA\n",
      "0\n",
      "[CLS] children should be able to dress as they wish, within reason, at school rather than being rest\n",
      "[CLS] school uniform is harming the student's self expression [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [P\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print( tokenizer.decode(toks1_input_dev[i])[0:100] ) \n",
    "  print( tokenizer.decode(toks2_input_dev[i])[0:100] )\n",
    "  print(labels_dev[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "inputs_train = (\n",
    "   toks1_input_train,\n",
    "   atts1_input_train,\n",
    "   toks2_input_train,\n",
    "   atts2_input_train\n",
    "  )\n",
    "input_dataset_train = tf.data.Dataset.from_tensor_slices( inputs_train )\n",
    "output_dataset_train = tf.data.Dataset.from_tensor_slices( labels_train )\n",
    "dataset_train = tf.data.Dataset.zip( (input_dataset_train, output_dataset_train) )\n",
    "\n",
    "inputs_dev = (\n",
    "   toks1_input_dev,\n",
    "   atts1_input_dev,\n",
    "   toks2_input_dev,\n",
    "   atts2_input_dev\n",
    "  )\n",
    "input_dataset_dev = tf.data.Dataset.from_tensor_slices( inputs_dev )\n",
    "output_dataset_dev = tf.data.Dataset.from_tensor_slices( labels_dev )\n",
    "dataset_dev = tf.data.Dataset.zip( (input_dataset_dev, output_dataset_dev) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7efd44af75e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7efd44af75e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import Lambda\n",
    "from numpy import dtype\n",
    "from transformers import TFBertModel\n",
    "\n",
    "bert = TFBertModel.from_pretrained(\"bert-base-uncased\", output_attentions=False, use_cache=False) #.bert\n",
    "toks1=tf.keras.Input(shape=(MAX_LEN,), dtype='int32')\n",
    "atts1=tf.keras.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "out1=bert(input_ids=toks1,attention_mask=atts1)\n",
    "\n",
    "toks2=tf.keras.Input(shape=(MAX_LEN,), dtype='int32')\n",
    "atts2=tf.keras.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "out2=bert(input_ids=toks2,attention_mask=atts2)\n",
    "\n",
    "# prima provavamo a fare la media\n",
    "# mean1=tf.reduce_mean(out1[0],1)\n",
    "# mean2=tf.reduce_mean(out2[0],1)\n",
    "\n",
    "# #########Comment this block if objective is cosine similarity calculation\n",
    "# cosine_similarity=tf.keras.layers.Dot(axes=1,normalize=True)\n",
    "# preds=cosine_similarity([mean1,mean2])\n",
    "\n",
    "cls1 = out1[0][:,0,:]\n",
    "cls2 = out2[0][:,0,:]\n",
    "\n",
    "#########Comment this block if objective is cosine similarity calculation\n",
    "cosine_similarity=tf.keras.layers.Dot(axes=1,normalize=True, dtype=\"float16\")\n",
    "preds=cosine_similarity([cls1,cls2])\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[toks1,atts1,toks2,atts2], outputs=preds)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-05), metrics=['mse'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 204/1290 [===>..........................] - ETA: 17:15 - loss: 0.0725 - mse: 0.0725"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# max_queue_size=2\n",
    "h = model.fit(dataset_train.batch(16), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "preds_train = model.predict(inputs_train)\n",
    "pd.DataFrame(preds_train).to_csv('./preds_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dev = model.predict(inputs_dev)\n",
    "pd.DataFrame(preds_dev).to_csv('./preds_dev.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3458 3458\n",
      "\n",
      "ֿ** loading task data:\n",
      "\t(-1, 'We should abandon the use of school uniform'): loaded 121 arguments and 5 key points\n",
      "\t(-1, 'We should abolish the right to keep and bear arms'): loaded 123 arguments and 4 key points\n",
      "\t(-1, 'We should adopt an austerity regime'): loaded 108 arguments and 5 key points\n",
      "\t(-1, 'We should end affirmative action'): loaded 108 arguments and 3 key points\n",
      "\t(1, 'We should abandon the use of school uniform'): loaded 117 arguments and 5 key points\n",
      "\t(1, 'We should abolish the right to keep and bear arms'): loaded 110 arguments and 4 key points\n",
      "\t(1, 'We should adopt an austerity regime'): loaded 126 arguments and 5 key points\n",
      "\t(1, 'We should end affirmative action'): loaded 119 arguments and 5 key points\n",
      "\n",
      "ֿ** loading predictions:\n",
      "\tloaded predictions for 932 arguments\n",
      "\n",
      "** predictions analysis:\n",
      "\t(-1, 'We should abandon the use of school uniform'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (121/121)\n",
      "\t(-1, 'We should abolish the right to keep and bear arms'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (123/123)\n",
      "\t(-1, 'We should adopt an austerity regime'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (108/108)\n",
      "\t(-1, 'We should end affirmative action'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (108/108)\n",
      "\t(1, 'We should abandon the use of school uniform'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (117/117)\n",
      "\t(1, 'We should abolish the right to keep and bear arms'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (110/110)\n",
      "\t(1, 'We should adopt an austerity regime'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (126/126)\n",
      "\t(1, 'We should end affirmative action'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (119/119)\n",
      "\n",
      "** running evalution:\n",
      "mAP strict= 0.7001732831115468 ; mAP relaxed = 0.7001732831115468\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "preds_dev = pd.read_csv('./preds_dev.csv')\n",
    "labels = pd.read_csv('./KPA_2021_shared_task/kpm_data/labels_dev.csv')\n",
    "\n",
    "print(len(preds_dev), len(labels))\n",
    "\n",
    "jsons = {}\n",
    "for i, line in preds_dev.T.iteritems():\n",
    "  pred = line[0]\n",
    "  line_labels = labels.iloc[i]\n",
    "  arg_id = line_labels['arg_id']\n",
    "  kp_id = line_labels['key_point_id']\n",
    "  \n",
    "  try:\n",
    "    jsons[arg_id][kp_id] = pred\n",
    "  except KeyError:\n",
    "    jsons[arg_id] = {kp_id:pred}\n",
    "\n",
    "import json\n",
    "with open('preds_dev.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(jsons, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "from KPA_2021_shared_task.code.track_1_kp_matching import load_kpm_data, get_predictions, evaluate_predictions\n",
    "\n",
    "gold_data_dir = './KPA_2021_shared_task/kpm_data/'\n",
    "predictions_file = './preds_dev.json'\n",
    "\n",
    "arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=\"dev\")\n",
    "\n",
    "merged_df = get_predictions(predictions_file, labels_df, arg_df, kp_df)\n",
    "evaluate_predictions(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20635 20635\n",
      "\n",
      "ֿ** loading task data:\n",
      "\t(-1, 'Assisted suicide should be a criminal offence'): loaded 121 arguments and 4 key points\n",
      "\t(-1, 'Homeschooling should be banned'): loaded 129 arguments and 6 key points\n",
      "\t(-1, 'The vow of celibacy should be abandoned'): loaded 112 arguments and 6 key points\n",
      "\t(-1, 'We should abandon marriage'): loaded 111 arguments and 5 key points\n",
      "\t(-1, 'We should abolish capital punishment'): loaded 110 arguments and 5 key points\n",
      "\t(-1, 'We should abolish intellectual property rights'): loaded 123 arguments and 4 key points\n",
      "\t(-1, 'We should adopt atheism'): loaded 123 arguments and 3 key points\n",
      "\t(-1, 'We should adopt libertarianism'): loaded 113 arguments and 5 key points\n",
      "\t(-1, 'We should ban human cloning'): loaded 123 arguments and 5 key points\n",
      "\t(-1, 'We should ban private military companies'): loaded 106 arguments and 4 key points\n",
      "\t(-1, 'We should ban the use of child actors'): loaded 121 arguments and 5 key points\n",
      "\t(-1, 'We should close Guantanamo Bay detention camp'): loaded 113 arguments and 3 key points\n",
      "\t(-1, 'We should end mandatory retirement'): loaded 78 arguments and 2 key points\n",
      "\t(-1, 'We should fight for the abolition of nuclear weapons'): loaded 80 arguments and 1 key points\n",
      "\t(-1, 'We should fight urbanization'): loaded 106 arguments and 3 key points\n",
      "\t(-1, 'We should introduce compulsory voting'): loaded 129 arguments and 4 key points\n",
      "\t(-1, 'We should legalize cannabis'): loaded 109 arguments and 5 key points\n",
      "\t(-1, 'We should legalize prostitution'): loaded 99 arguments and 4 key points\n",
      "\t(-1, 'We should legalize sex selection'): loaded 126 arguments and 6 key points\n",
      "\t(-1, 'We should prohibit flag burning'): loaded 118 arguments and 2 key points\n",
      "\t(-1, 'We should prohibit women in combat'): loaded 130 arguments and 6 key points\n",
      "\t(-1, 'We should subsidize journalism'): loaded 110 arguments and 3 key points\n",
      "\t(-1, 'We should subsidize space exploration'): loaded 115 arguments and 4 key points\n",
      "\t(-1, 'We should subsidize vocational education'): loaded 95 arguments and 4 key points\n",
      "\t(1, 'Assisted suicide should be a criminal offence'): loaded 125 arguments and 6 key points\n",
      "\t(1, 'Homeschooling should be banned'): loaded 115 arguments and 4 key points\n",
      "\t(1, 'The vow of celibacy should be abandoned'): loaded 122 arguments and 5 key points\n",
      "\t(1, 'We should abandon marriage'): loaded 125 arguments and 4 key points\n",
      "\t(1, 'We should abolish capital punishment'): loaded 126 arguments and 5 key points\n",
      "\t(1, 'We should abolish intellectual property rights'): loaded 91 arguments and 6 key points\n",
      "\t(1, 'We should adopt atheism'): loaded 105 arguments and 4 key points\n",
      "\t(1, 'We should adopt libertarianism'): loaded 124 arguments and 4 key points\n",
      "\t(1, 'We should ban human cloning'): loaded 122 arguments and 4 key points\n",
      "\t(1, 'We should ban private military companies'): loaded 126 arguments and 5 key points\n",
      "\t(1, 'We should ban the use of child actors'): loaded 123 arguments and 5 key points\n",
      "\t(1, 'We should close Guantanamo Bay detention camp'): loaded 128 arguments and 6 key points\n",
      "\t(1, 'We should end mandatory retirement'): loaded 122 arguments and 5 key points\n",
      "\t(1, 'We should fight for the abolition of nuclear weapons'): loaded 116 arguments and 2 key points\n",
      "\t(1, 'We should fight urbanization'): loaded 127 arguments and 4 key points\n",
      "\t(1, 'We should introduce compulsory voting'): loaded 115 arguments and 3 key points\n",
      "\t(1, 'We should legalize cannabis'): loaded 137 arguments and 6 key points\n",
      "\t(1, 'We should legalize prostitution'): loaded 132 arguments and 5 key points\n",
      "\t(1, 'We should legalize sex selection'): loaded 118 arguments and 5 key points\n",
      "\t(1, 'We should prohibit flag burning'): loaded 99 arguments and 2 key points\n",
      "\t(1, 'We should prohibit women in combat'): loaded 107 arguments and 5 key points\n",
      "\t(1, 'We should subsidize journalism'): loaded 118 arguments and 4 key points\n",
      "\t(1, 'We should subsidize space exploration'): loaded 132 arguments and 5 key points\n",
      "\t(1, 'We should subsidize vocational education'): loaded 128 arguments and 4 key points\n",
      "\n",
      "ֿ** loading predictions:\n",
      "\tloaded predictions for 5583 arguments\n",
      "\n",
      "** predictions analysis:\n",
      "\t(-1, 'Assisted suicide should be a criminal offence'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (121/121)\n",
      "\t(-1, 'Homeschooling should be banned'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (129/129)\n",
      "\t(-1, 'The vow of celibacy should be abandoned'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (112/112)\n",
      "\t(-1, 'We should abandon marriage'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (111/111)\n",
      "\t(-1, 'We should abolish capital punishment'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (110/110)\n",
      "\t(-1, 'We should abolish intellectual property rights'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (123/123)\n",
      "\t(-1, 'We should adopt atheism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (123/123)\n",
      "\t(-1, 'We should adopt libertarianism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (113/113)\n",
      "\t(-1, 'We should ban human cloning'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (123/123)\n",
      "\t(-1, 'We should ban private military companies'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (106/106)\n",
      "\t(-1, 'We should ban the use of child actors'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (121/121)\n",
      "\t(-1, 'We should close Guantanamo Bay detention camp'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (113/113)\n",
      "\t(-1, 'We should end mandatory retirement'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (78/78)\n",
      "\t(-1, 'We should fight for the abolition of nuclear weapons'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (80/80)\n",
      "\t(-1, 'We should fight urbanization'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (106/106)\n",
      "\t(-1, 'We should introduce compulsory voting'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (129/129)\n",
      "\t(-1, 'We should legalize cannabis'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (109/109)\n",
      "\t(-1, 'We should legalize prostitution'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (99/99)\n",
      "\t(-1, 'We should legalize sex selection'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (126/126)\n",
      "\t(-1, 'We should prohibit flag burning'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (118/118)\n",
      "\t(-1, 'We should prohibit women in combat'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (130/130)\n",
      "\t(-1, 'We should subsidize journalism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (110/110)\n",
      "\t(-1, 'We should subsidize space exploration'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (115/115)\n",
      "\t(-1, 'We should subsidize vocational education'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (95/95)\n",
      "\t(1, 'Assisted suicide should be a criminal offence'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (125/125)\n",
      "\t(1, 'Homeschooling should be banned'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (115/115)\n",
      "\t(1, 'The vow of celibacy should be abandoned'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (122/122)\n",
      "\t(1, 'We should abandon marriage'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (125/125)\n",
      "\t(1, 'We should abolish capital punishment'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (126/126)\n",
      "\t(1, 'We should abolish intellectual property rights'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (91/91)\n",
      "\t(1, 'We should adopt atheism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (105/105)\n",
      "\t(1, 'We should adopt libertarianism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (124/124)\n",
      "\t(1, 'We should ban human cloning'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (122/122)\n",
      "\t(1, 'We should ban private military companies'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (126/126)\n",
      "\t(1, 'We should ban the use of child actors'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (123/123)\n",
      "\t(1, 'We should close Guantanamo Bay detention camp'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (128/128)\n",
      "\t(1, 'We should end mandatory retirement'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (122/122)\n",
      "\t(1, 'We should fight for the abolition of nuclear weapons'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (116/116)\n",
      "\t(1, 'We should fight urbanization'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (127/127)\n",
      "\t(1, 'We should introduce compulsory voting'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (115/115)\n",
      "\t(1, 'We should legalize cannabis'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (137/137)\n",
      "\t(1, 'We should legalize prostitution'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (132/132)\n",
      "\t(1, 'We should legalize sex selection'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (118/118)\n",
      "\t(1, 'We should prohibit flag burning'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (99/99)\n",
      "\t(1, 'We should prohibit women in combat'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (107/107)\n",
      "\t(1, 'We should subsidize journalism'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (118/118)\n",
      "\t(1, 'We should subsidize space exploration'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (132/132)\n",
      "\t(1, 'We should subsidize vocational education'):\n",
      "\t\tsubmitted matched for 1.0 of the arguments (128/128)\n",
      "\n",
      "** running evalution:\n",
      "mAP strict= 0.8639244890354627 ; mAP relaxed = 0.8639244890354627\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "preds_train = pd.read_csv('./preds_train.csv')\n",
    "labels = pd.read_csv('./KPA_2021_shared_task/kpm_data/labels_train.csv')\n",
    "\n",
    "print(len(preds_train), len(labels))\n",
    "\n",
    "jsons = {}\n",
    "for i, line in preds_train.T.iteritems():\n",
    "  pred = line[0]\n",
    "  line_labels = labels.iloc[i]\n",
    "  arg_id = line_labels['arg_id']\n",
    "  kp_id = line_labels['key_point_id']\n",
    "  \n",
    "  try:\n",
    "    jsons[arg_id][kp_id] = pred\n",
    "  except KeyError:\n",
    "    jsons[arg_id] = {kp_id:pred}\n",
    "\n",
    "import json\n",
    "with open('preds_train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(jsons, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "from KPA_2021_shared_task.code.track_1_kp_matching import load_kpm_data, get_predictions, evaluate_predictions\n",
    "\n",
    "gold_data_dir = './KPA_2021_shared_task/kpm_data/'\n",
    "predictions_file = './preds_train.json'\n",
    "\n",
    "arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=\"train\")\n",
    "\n",
    "merged_df = get_predictions(predictions_file, labels_df, arg_df, kp_df)\n",
    "evaluate_predictions(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,p in enumerate(preds_train):\n",
    "  if i<100:\n",
    "    print(labels_dev[i],p)\n",
    "  else: \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean1=tf.reduce_mean(out1[0],1)\n",
    "# mean2=tf.reduce_mean(out2[0],1)\n",
    "cls1 = out1[0][:,0,:]\n",
    "cls2 = out2[0][:,0,:]\n",
    "\n",
    "#########Comment this block if objective is cosine similarity calculation\n",
    "cosine_similarity=tf.keras.layers.Dot(axes=1,normalize=True)\n",
    "preds=cosine_similarity([cls1,cls2])\n",
    "\n",
    "\n",
    "model = Model(inputs=[toks1,atts1,toks2,atts2], outputs=preds)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1[0][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(768)\n",
    "b = np.copy(a)\n",
    "np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "######## BALANCE THE DATASET ########\n",
    "\n",
    "# get a subset of the possible indices\n",
    "rng = np.random.default_rng()\n",
    "size = 15000\n",
    "indeces1 = np.random.randint(len(toks1_input), size=size)\n",
    "indeces2 = np.random.randint(len(toks1_input), size=size)\n",
    "\n",
    "# get a random subset of the arguments\n",
    "new_toks1 = np.take( toks1_input, indeces1 , axis=0)\n",
    "new_atts1 = np.take( atts1_input, indeces1 , axis=0)\n",
    "\n",
    "# get a random subset of the kps\n",
    "new_toks2 = np.take( toks2_input, indeces2 , axis=0)\n",
    "new_atts2 = np.take( atts2_input, indeces2 , axis=0)\n",
    "\n",
    "# add the random subsets at the end \n",
    "toks1_input = np.vstack(( toks1_input, new_toks1))\n",
    "atts1_input = np.vstack(( atts1_input, new_atts1))\n",
    "\n",
    "toks2_input = np.vstack(( toks2_input, new_toks2))\n",
    "atts2_input = np.vstack(( atts2_input, new_atts2))\n",
    "\n",
    "# assume they don't match\n",
    "labels = np.hstack((labels, np.array([-1]*size) ))\n",
    "\n",
    "# shuffle the new dataset:\n",
    "\n",
    "# take a vector [0,1,2,....,n]\n",
    "order = np.arange(0,len(labels))\n",
    "\n",
    "# shuffle it, so it becomes a random order\n",
    "rng.shuffle(order)\n",
    "\n",
    "# resave the elements in the random order\n",
    "b_toks1_input = np.take(toks1_input,order, axis=0)\n",
    "b_atts1_input = np.take(atts1_input,order, axis=0)\n",
    "\n",
    "b_toks2_input = np.take(toks2_input,order, axis=0)\n",
    "b_atts2_input = np.take(atts2_input,order, axis=0)\n",
    "\n",
    "b_labels = np.take(labels,order, axis=0)\n",
    "\n",
    "\n",
    "print(b_toks1_input.shape,b_atts1_input.shape,b_toks2_input.shape,b_atts2_input.shape, b_labels.shape)\n",
    "sum(labels==1),           sum(labels==-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('usr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
